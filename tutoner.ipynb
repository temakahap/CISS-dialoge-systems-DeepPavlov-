{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize named entities on news data with CNN\n",
    "\n",
    "In this tutorial, you will use a convolutional neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction of entities from text such as persons, organizations, locations, etc. In this task you will experiment with recognition of named entities in different news texts from CoNLL-2003 dataset.\n",
    "\n",
    "For example, we want to extract person and organization names from the text. Then for the input text:\n",
    "\n",
    "    Ian Goodfellow works for Google Brain\n",
    "\n",
    "a NER model needs to provide the following sequence of tags:\n",
    "\n",
    "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "\n",
    "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called **BIO markup**. This markup is introduced for distinguishing of consequent entities with similar types.\n",
    "\n",
    "A solution of the task will be based on neural networks, particularly, on Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `/data`. The download util from the library is used to download and extract the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 16:38:05.790 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 208: Starting new HTTP connection (1): lnsigo.mipt.ru\n",
      "2018-07-05 16:38:05.956 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 396: http://lnsigo.mipt.ru:80 \"GET /export/deeppavlov_data/conll2003_v2.tar.gz HTTP/1.1\" 200 957092\n",
      "2018-07-05 16:38:05.958 INFO in 'deeppavlov.core.data.utils'['utils'] at line 65: Downloading from http://lnsigo.mipt.ru/export/deeppavlov_data/conll2003_v2.tar.gz to /home/temkahap/Рабочий стол/CISS/DeepPavlov/HOMEWORK/data/conll2003_v2.tar.gz\n",
      "100%|██████████| 957k/957k [00:01<00:00, 502kB/s] \n",
      "2018-07-05 16:38:07.869 INFO in 'deeppavlov.core.data.utils'['utils'] at line 149: Extracting data/conll2003_v2.tar.gz archive into data\n"
     ]
    }
   ],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://lnsigo.mipt.ru/export/deeppavlov_data/conll2003_v2.tar.gz', 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CoNLL-2003 Named Entity Recognition corpus\n",
    "\n",
    "We will work with a corpus which contains tweets with NE tags. A typical file with NER data contains lines with pairs of tokens (word or punctuation symbol) and tags separated by a whitespace. In many cases additional information such as POS tags is included. Different documents are separated with lines **started** with **-DOCSTART-** token. Different sentences are separated with an empty line. Example:\n",
    "\n",
    "    -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "\n",
    "    Peter NNP B-NP B-PER\n",
    "    Blackburn NNP I-NP I-PER\n",
    "\n",
    "In this tutorial we will focus only on tokens and tags (first and last elements of the line) and drop POS information located in between.\n",
    "\n",
    "We start with using the *Conll2003DatasetReader* class that provides functionality for reading the dataset. It returns a dictionary with fields *train*, *test*, and *valid*. At each field a list of samples is stored. Each sample is a tuple of tokens and tags. Both tokens and tags are lists. The following example depicts the structure that should be returned by *read* method:\n",
    "\n",
    "    {'train': [(['Mr.', 'Dwag', 'are', 'derping', 'around'], ['B-PER', 'I-PER', 'O', 'O', 'O']), ....],\n",
    "     'valid': [...],\n",
    "     'test': [...]}\n",
    "\n",
    "There are three separate parts of the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model.\n",
    " \n",
    "\n",
    "Each of these parts is stored in a separate txt file.\n",
    "\n",
    "We will use [Conll2003DatasetReader](https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/dataset_readers/conll2003_reader.py) from the library to read the data from text files to the format described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_readers.conll2003_reader import Conll2003DatasetReader\n",
    "dataset = Conll2003DatasetReader().read('data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DOCSTART>\tO\n",
      "\n",
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n",
      "\n",
      "Peter\tB-PER\n",
      "Blackburn\tI-PER\n",
      "\n",
      "BRUSSELS\tB-LOC\n",
      "1996-08-22\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset['train'][:4]:\n",
    "    for token, tag in zip(*sample):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dictionaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: index of the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Token indices will be used to find the corresponding rows in embedding matrix. The mapping for tags will be used to create one-hot ground-truth probability distribution vectors to compute the loss at the output of the network.\n",
    "\n",
    "The [SimpleVocabulary](https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/core/data/simple_vocab.py) implemented in the library will be used to perform those mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build dictionaries for tokens and tags. Sometimes there are special tokens in vocabularies, for instance an unknown word token, which is used every time we encounter an out-of-vocabulary word. In our case the only special token will be`<UNK>` for out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 16:38:11.673 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 53: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n",
      "2018-07-05 16:38:11.677 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 53: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<UNK>']\n",
    "\n",
    "token_vocab = SimpleVocabulary(special_tokens, save_path='model/token.dict')\n",
    "tag_vocab = SimpleVocabulary(save_path='model/tag.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the vocabularies on the train part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tokens_by_sentences = [tokens for tokens, tags in dataset['train']]\n",
    "all_tags_by_sentences = [tags for tokens, tags in dataset['train']]\n",
    "\n",
    "token_vocab.fit(all_tokens_by_sentences)\n",
    "tag_vocab.fit(all_tags_by_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get the indices. Keep in mind that we are working with batches of the following structure:\n",
    "    \n",
    "    [['utt0_tok0', 'utt1_tok1', ...], ['utt1_tok0', 'utt1_tok1', ...], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10167, 6, 168, 7, 6097, 5518, 1865]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vocab([['How', 'to', 'do', 'a', 'barrel', 'roll', '?']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [3, 5]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vocab([['O', 'O', 'O'], ['B-ORG', 'I-ORG']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try converting from indices to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['did',\n",
       "  'first',\n",
       "  'We',\n",
       "  'Russian',\n",
       "  '4.',\n",
       "  'now',\n",
       "  \"n't\",\n",
       "  'singles',\n",
       "  ')',\n",
       "  'Clinton']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "token_vocab([np.random.randint(0, 512, size=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Iterator\n",
    "\n",
    "Neural Networks are usually trained on batches of examples. It means that weight updates of the network are based on several sequences at every step. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<UNK>` token. Likewise, token tags must also be padded. It is also a good practice to provide RNN with sequence lengths, so that it can skip computations for padding parts. We provide the batching function *batches_generator* readily available for you to save time. \n",
    "\n",
    "An important concept in the batch generation is shuffling. Shuffling is taking sample from the dataset in random order. It is important to train on shuffled data because large number of consequetive samples of the same class may distort the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.data_learning_iterator import DataLearningIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset iterator for the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_iterator = DataLearningIterator(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['for', ',', 'against', ',', 'points', ')'],\n",
       "  ['Iran',\n",
       "   'has',\n",
       "   'asked',\n",
       "   'Germany',\n",
       "   'to',\n",
       "   'extradite',\n",
       "   'its',\n",
       "   'former',\n",
       "   'president',\n",
       "   'Abolhassan',\n",
       "   'Banisadr',\n",
       "   'for',\n",
       "   'alleged',\n",
       "   'hijacking',\n",
       "   ',',\n",
       "   'an',\n",
       "   'Iranian',\n",
       "   'embassy',\n",
       "   'spokesman',\n",
       "   'said',\n",
       "   'on',\n",
       "   'Wednesday',\n",
       "   '.']),\n",
       " (['O', 'O', 'O', 'O', 'O', 'O'],\n",
       "  ['B-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'I-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-MISC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_iterator.gen_batches(2, shuffle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing about generating training data. We need to produce a binary mask which is the one where tokens present and zero elsewhere. This mask will stop backpropagation through paddings. An instance of such mask:\n",
    "\n",
    "    [[1, 1, 0, 0, 0],\n",
    "     [1, 1, 1, 1, 1]]\n",
    " For the sentences in batch:\n",
    "\n",
    "     [['The', 'roof'],\n",
    "      ['This', 'is', 'my', 'domain', '!']]\n",
    "\n",
    "The Mask preprocessing component from the library will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeppavlov.models.preprocessors.mask import Mask\n",
    "get_mask = Mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mask([['Try', 'to', 'get', 'the', 'mask'], ['Check', 'paddings']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Convolutional Neural Network\n",
    "\n",
    "This is the most important part of the assignment. Here we will specify the network architecture based on `TensorFlow` building blocks. It's fun and easy as a lego constructor! We will create an Convolutional Neural Network (CNN) which will produce the probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use CNN. Dense layer will be used on top to perform tag classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An essential part of almost every network in NLP domain is embeddings of the words. We pass the text to the network as a series of tokens. Each token is represented by its index. For every token (index) we have a vector. In total the vectors form an embedding matrix. This matrix can be either pretrained using some common algorithm like Skip-Gram or CBOW or it can be initialized by random values and trained along with other parameters of the network. In this tutorial we will follow the second alternative.\n",
    "\n",
    "We need to build a function that takes the tensor of token indices with shape [batch_size, num_tokens] and for each index in this matrix it retrieves a vector from the embedding matrix, corresponding to that index. That results in a new tensor with sahpe [batch_size, num_tokens, emb_dim]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings(indices, vocabulary_size, emb_dim):\n",
    "    # Initialize the random gaussian matrix with dimensions [vocabulary_size, embedding_dimension]\n",
    "    # The **VARIANCE** of the random samples must be 1 / embedding_dimension\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    emb_mat = tf.Variable(tf.random_normal(shape=[vocabulary_size, emb_dim], \n",
    "                                          mean=0.0, stddev = 1.0 / emb_dim), \n",
    "                                          trainable=True, dtype=tf.float32)\n",
    "    emb = tf.nn.embedding_lookup(emb_mat, indices)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings are ready to deploy\n"
     ]
    }
   ],
   "source": [
    "indices = [[0, 1, 2]] # batch of indices of tokens\n",
    "vocab_size = 5\n",
    "emb_dim = 100\n",
    "\n",
    "emb = get_embeddings(indices, vocab_size, emb_dim)\n",
    "emb_shape = emb.get_shape().as_list()\n",
    "assert emb_shape[0] == 1\n",
    "assert emb_shape[1] == 3\n",
    "assert emb_shape[2] == emb_dim\n",
    "print('Embeddings are ready to deploy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The body of the network is the convolutional layers. The basic idea behind convolutions is to apply the same dense layer to every n consecutive samples (tokens in our case). A simplified case is depicted below.\n",
    "\n",
    "<img src=\"conv.png\" width=\"400\">\n",
    "\n",
    "Here number of input and output features equals to 1.\n",
    "\n",
    "Let's try it on a toy example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d_32/BiasAdd:0\", shape=(2, 3, 200), dtype=float32)\n",
      "Tensor(\"random_normal_15:0\", shape=(2, 10, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with shape [batch_size, number_of_tokens, number_of_features]\n",
    "x = tf.random_normal(shape=[2, 10, 100])\n",
    "y = tf.layers.conv1d(x, filters=200, kernel_size=8)\n",
    "print(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, due to the abscence of zero padding (zeros on in the beginning and in the end of input) the size of resulting tensor along the token dimension is reduced. To use padding and preserve the dimensionality along the convolution dimension pass padding='same' parameter to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d_33/BiasAdd:0\", shape=(2, 10, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_with_padding = tf.layers.conv1d(x, filters=200, kernel_size=8, padding='same')\n",
    "print(y_with_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now stack a number of layers with dimensionality given in n_hidden_list (list of numbers of hidden units in each layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(units, n_hidden_list, cnn_filter_width, activation=tf.nn.relu):\n",
    "    # Use activation(units) to apply activation to units\n",
    "    \n",
    "    ######################################\n",
    "    ########## YOUR CODE HERE ############\n",
    "    ######################################\n",
    "    for i in n_hidden_list:\n",
    "        units = tf.layers.conv1d(units, filters=i, kernel_size=cnn_filter_width, \n",
    "                                 padding='same')\n",
    "        units = activation(units)\n",
    "    return units\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet is ready to deploy\n"
     ]
    }
   ],
   "source": [
    "n_hidden_list = [10, 20]\n",
    "x = tf.Variable(np.random.randn(2, 10, 32), tf.float32)# tensor with dimensions [batch_size, number_of_tokens, number_of_features]\n",
    "cnn_filter_width = 3\n",
    "y = conv_net(x, n_hidden_list, cnn_filter_width)\n",
    "output_shape = y.get_shape().as_list()\n",
    "assert output_shape[0] == 2\n",
    "assert output_shape[1] == 10\n",
    "assert output_shape[2] == n_hidden_list[-1]\n",
    "print('ConvNet is ready to deploy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common loss for the classification task is cross-entropy. Why classification? Because for each token the network must decide which tag to predict. The cross-entropy has the following form:\n",
    "\n",
    "$$ H(P, Q) = -E_{x \\sim P} log Q(x) $$\n",
    "\n",
    "It measures the dissimilarity between the ground truth distribution over the classes and predicted distribution. In the most of the cases ground truth distribution is one-hot. Luckily this loss is already [implemented](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2) in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"softmax_cross_entropy_with_logits_20/Reshape_2:0\", shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# The logits\n",
    "l = tf.random_normal([1, 4, 3]) # shape [batch_size, number_of_tokens, number of classes]\n",
    "indices = tf.placeholder(tf.int32, [1, 4])\n",
    "\n",
    "# Make one-hot distribution from indices for 3 types of tag\n",
    "p = tf.one_hot(indices, depth=3)\n",
    "loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=p, logits=l)\n",
    "print(loss_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sentences in the batch have same length and we pad the each sentence to the maximal lendth. So there are paddings at the end and pushing the network to predict those paddings usually results in deteriorated quallity. Then we need to multiply the loss tensor by binary mask to prevent gradient flow from the paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul_20:0' shape=(1, 4) dtype=float32>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = tf.placeholder(tf.float32, shape=[1, 4])\n",
    "loss_tensor *= mask\n",
    "loss_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step to do is to compute the mean value of the loss tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(loss_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define your own function that returns a scalar masked cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def masked_cross_entropy(logits, label_indices, number_of_tags, mask):\n",
    "    \n",
    "    ######################################\n",
    "    ########## YOUR CODE HERE ############\n",
    "    ######################################\n",
    "    p = tf.one_hot(label_indices, depth=number_of_tags)\n",
    "\n",
    "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=p, logits=logits)\n",
    "    loss_tensor *= mask\n",
    "    loss = tf.reduce_mean(loss_tensor)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all works fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.placeholder(tf.float32, shape=[2, 3, 10])\n",
    "label_indices = tf.placeholder(tf.int32, shape=[2, 3])\n",
    "number_of_tags = 10\n",
    "mask = tf.placeholder(tf.float32, shape=[2, 3])\n",
    "\n",
    "loss = masked_cross_entropy(logits, label_indices, number_of_tags, mask)\n",
    "\n",
    "assert len(loss.get_shape().as_list()) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put everything into a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class NerNetwork:\n",
    "    def __init__(self,\n",
    "                 n_tokens,\n",
    "                 n_tags,\n",
    "                 token_emb_dim=100,\n",
    "                 n_hidden_list=(128,),\n",
    "                 cnn_filter_width=7,\n",
    "                 use_batch_norm=False,\n",
    "                 embeddings_dropout=False,\n",
    "                 top_dropout=False,\n",
    "                 **kwargs):\n",
    "        \n",
    "        # ================ Building inputs =================\n",
    "        print (\"Started building iputs.\")\n",
    "        self.learning_rate_ph = tf.placeholder(tf.float32, [])\n",
    "        self.dropout_keep_ph = tf.placeholder(tf.float32, [])\n",
    "        self.token_ph = tf.placeholder(tf.int32, [None, None], name='token_ind_ph')\n",
    "        self.mask_ph = tf.placeholder(tf.float32, [None, None], name='Mask_ph')\n",
    "        self.y_ph = tf.placeholder(tf.int32, [None, None], name='y_ph')\n",
    "        \n",
    "        # ================== Building the network ==================\n",
    "        \n",
    "        # Now embedd the indices of tokens using token_emb_dim function\n",
    "        # this should be like\n",
    "        \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        emb = get_embeddings(self.token_ph, n_tokens, token_emb_dim)\n",
    "        ######################################\n",
    "        print (\"emb\", emb)\n",
    "        emb = tf.nn.dropout(emb, self.dropout_keep_ph, (tf.shape(emb)[0], 1, tf.shape(emb)[2]))\n",
    "        \n",
    "        # Build a multilayer CNN on top of the embeddings.\n",
    "        # The number of units in the each layer must match\n",
    "        # corresponding number from n_hidden_list.\n",
    "        # Use ReLU activation \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        units = conv_net(emb, n_hidden_list, cnn_filter_width, activation=tf.nn.relu)\n",
    "        ######################################\n",
    "        units = tf.nn.dropout(units, self.dropout_keep_ph, (tf.shape(units)[0], 1, tf.shape(units)[2]))\n",
    "        logits = tf.layers.dense(units, n_tags, activation=None)\n",
    "        self.predictions = tf.argmax(logits, 2)\n",
    "        \n",
    "        # ================= Loss and train ops =================\n",
    "        # Use cross-entropy loss. \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        self.loss = masked_cross_entropy(logits, self.y_ph, n_tags, self.mask_ph)\n",
    "        ######################################\n",
    "\n",
    "        # Create a training operation to update the network parameters.\n",
    "        # We purpose to use the Adam optimizer as it work fine for the\n",
    "        # most of the cases. Check tf.train to find an implementation.\n",
    "        # Put the train operation to the attribute self.train_op\n",
    "        \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate_ph).minimize(self.loss)\n",
    "        ######################################\n",
    "\n",
    "        # ================= Initialize the session =================\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def __call__(self, tok_batch, mask_batch):\n",
    "        feed_dict = {self.token_ph: tok_batch,\n",
    "                     self.mask_ph: mask_batch,\n",
    "                     self.dropout_keep_ph: 1.0}\n",
    "\n",
    "        return self.sess.run(self.predictions, feed_dict)\n",
    "\n",
    "    def train_on_batch(self, tok_batch, tag_batch, mask_batch, dropout_keep_prob, learning_rate):\n",
    "        feed_dict = {self.token_ph: tok_batch,\n",
    "                     self.y_ph: tag_batch,\n",
    "                     self.mask_ph: mask_batch,\n",
    "                     self.dropout_keep_ph: dropout_keep_prob,\n",
    "                     self.learning_rate_ph: learning_rate}\n",
    "        self.sess.run(self.train_op, feed_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an instance of the NerNetwork class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started building iputs.\n",
      "emb Tensor(\"embedding_lookup_16:0\", shape=(?, ?, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "nernet = NerNetwork(len(token_vocab),\n",
    "                    len(tag_vocab),\n",
    "                    n_hidden_list=[100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often want to check the score on validation part of the dataset every epoch. In most of the cases of NER tasks the classes are imbalanced. And the accuracy is not the best measure of performance. If we have 95% of 'O' tags, then a silly classifier that always predicts '0' gets 95% accuracy. To tackle this issue the F1-score is used. The $F_1$-score can be defined as:\n",
    "\n",
    "$$ F_1 =  \\frac{2 P R}{P + R}$$ \n",
    "\n",
    "where P is precision and R is recall.\n",
    "\n",
    "Let's write the evaluation function. We need to get all predictions for the given part of the dataset and compute $F_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeppavlov.models.ner.evaluation import precision_recall_f1\n",
    "# The function precision_recall_f1 takes two lists: y_true and y_predicted\n",
    "# the tag sequences for each sentences should be merged into one big list \n",
    "from deeppavlov.core.data.utils import zero_pad\n",
    "# zero_pad takes a batch of lists of token indices, pad it with zeros to the\n",
    "# maximal length and convert it to numpy matrix\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def eval_valid(network, batch_generator):\n",
    "    total_true = []\n",
    "    total_pred = []\n",
    "    for x, y_true in batch_generator:\n",
    "        \n",
    "        # Prepare token indices from tokens batch\n",
    "        x_inds = token_vocab(x)# YOUR CODE HERE\n",
    "\n",
    "        # Pad the indices batch with zeros\n",
    "        x_batch = zero_pad(x_inds)# YOUR CODE HERE\n",
    "        # Get the mask using get_maskz\n",
    "        mask = get_mask(x) # YOUR CODE HERE\n",
    "        # We call the instance of the NerNetwork because we have defined __call__ method\n",
    "        y_inds = network(x_batch, mask)\n",
    "        \n",
    "        # For every sentence in the batch extract all tags up to paddings (use length of x element)\n",
    "        y_inds = [y_[:len(x_)] for x_, y_ in zip(*[x, y_inds])]# YOUR CODE HERE\n",
    "        y_pred = tag_vocab(y_inds)\n",
    "\n",
    "        # Add fresh predictions \n",
    "        total_true.extend(chain(*y_true))\n",
    "        total_pred.extend(chain(*y_pred))\n",
    "    res = precision_recall_f1(total_true, total_pred, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 17:54:52.326 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 38471 phrases; correct: 612.\n",
      "\n",
      "precision:  1.59%; recall:  10.30%; FB1:  2.76\n",
      "\n",
      "\tLOC: precision:  2.21%; recall:  7.84%; F1:  3.45 6517\n",
      "\n",
      "\tMISC: precision:  1.33%; recall:  15.94%; F1:  2.45 11060\n",
      "\n",
      "\tORG: precision:  1.68%; recall:  11.19%; F1:  2.92 8931\n",
      "\n",
      "\tPER: precision:  1.43%; recall:  9.28%; F1:  2.48 11963\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_valid(nernet, data_iterator.gen_batches(16, data_type='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters for the training procedure. You might want to start with the following recommended values:\n",
    "- *batch_size*: 32;\n",
    "- n_epochs: 10;\n",
    "- starting value of *learning_rate*: 0.001\n",
    "- *learning_rate_decay*: a square root of 2;\n",
    "- *dropout_keep_probability* equal to 0.7 for training (typical values for dropout probability are ranging from 0.3 to 0.9).\n",
    "\n",
    "A very efficient technique for the learning rate managment is dropping learning rate after convergence. It is common to use dividers 2, 3, and 10 to drop the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32# YOUR HYPERPARAMETER HERE\n",
    "n_epochs = 10# YOUR HYPERPARAMETER HERE\n",
    "learning_rate = 0.001# YOUR HYPERPARAMETER HERE\n",
    "dropout_keep_prob = 0.7# YOUR HYPERPARAMETER HERE\n",
    "learning_rate_decay = np.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate through the dataset batch by batch and pass the data to the train op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 17:55:11.412 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5306 phrases; correct: 3201.\n",
      "\n",
      "precision:  60.33%; recall:  53.87%; FB1:  56.92\n",
      "\n",
      "\tLOC: precision:  74.34%; recall:  68.75%; F1:  71.44 1699\n",
      "\n",
      "\tMISC: precision:  57.69%; recall:  37.42%; F1:  45.39 598\n",
      "\n",
      "\tORG: precision:  47.48%; recall:  54.06%; F1:  50.56 1527\n",
      "\n",
      "\tPER: precision:  58.57%; recall:  47.12%; F1:  52.23 1482\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 17:55:30.836 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5199 phrases; correct: 4240.\n",
      "\n",
      "precision:  81.55%; recall:  71.36%; FB1:  76.12\n",
      "\n",
      "\tLOC: precision:  90.49%; recall:  81.87%; F1:  85.97 1662\n",
      "\n",
      "\tMISC: precision:  79.98%; recall:  75.81%; F1:  77.84 874\n",
      "\n",
      "\tORG: precision:  74.70%; recall:  69.35%; F1:  71.93 1245\n",
      "\n",
      "\tPER: precision:  78.07%; recall:  60.10%; F1:  67.91 1418\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 17:55:49.618 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5154 phrases; correct: 4304.\n",
      "\n",
      "precision:  83.51%; recall:  72.43%; FB1:  77.58\n",
      "\n",
      "\tLOC: precision:  90.84%; recall:  84.76%; F1:  87.69 1714\n",
      "\n",
      "\tMISC: precision:  85.08%; recall:  77.33%; F1:  81.02 838\n",
      "\n",
      "\tORG: precision:  78.94%; recall:  70.17%; F1:  74.30 1192\n",
      "\n",
      "\tPER: precision:  77.52%; recall:  59.34%; F1:  67.22 1410\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 17:56:08.750 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5161 phrases; correct: 4429.\n",
      "\n",
      "precision:  85.82%; recall:  74.54%; FB1:  79.78\n",
      "\n",
      "\tLOC: precision:  92.65%; recall:  84.43%; F1:  88.35 1674\n",
      "\n",
      "\tMISC: precision:  87.65%; recall:  79.28%; F1:  83.26 834\n",
      "\n",
      "\tORG: precision:  79.36%; recall:  72.56%; F1:  75.81 1226\n",
      "\n",
      "\tPER: precision:  82.27%; recall:  63.74%; F1:  71.83 1427\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 17:56:27.363 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5168 phrases; correct: 4438.\n",
      "\n",
      "precision:  85.87%; recall:  74.69%; FB1:  79.89\n",
      "\n",
      "\tLOC: precision:  91.57%; recall:  84.54%; F1:  87.91 1696\n",
      "\n",
      "\tMISC: precision:  86.52%; recall:  78.63%; F1:  82.39 838\n",
      "\n",
      "\tORG: precision:  79.92%; recall:  72.11%; F1:  75.81 1210\n",
      "\n",
      "\tPER: precision:  83.78%; recall:  64.77%; F1:  73.06 1424\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 17:56:45.788 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5206 phrases; correct: 4519.\n",
      "\n",
      "precision:  86.80%; recall:  76.05%; FB1:  81.07\n",
      "\n",
      "\tLOC: precision:  90.57%; recall:  86.28%; F1:  88.37 1750\n",
      "\n",
      "\tMISC: precision:  86.66%; recall:  78.20%; F1:  82.21 832\n",
      "\n",
      "\tORG: precision:  80.73%; recall:  72.78%; F1:  76.55 1209\n",
      "\n",
      "\tPER: precision:  87.42%; recall:  67.16%; F1:  75.96 1415\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 17:57:04.197 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5146 phrases; correct: 4479.\n",
      "\n",
      "precision:  87.04%; recall:  75.38%; FB1:  80.79\n",
      "\n",
      "\tLOC: precision:  92.64%; recall:  84.98%; F1:  88.64 1685\n",
      "\n",
      "\tMISC: precision:  89.12%; recall:  79.07%; F1:  83.79 818\n",
      "\n",
      "\tORG: precision:  81.37%; recall:  73.60%; F1:  77.29 1213\n",
      "\n",
      "\tPER: precision:  84.06%; recall:  65.26%; F1:  73.47 1430\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 17:57:23.140 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5252 phrases; correct: 4491.\n",
      "\n",
      "precision:  85.51%; recall:  75.58%; FB1:  80.24\n",
      "\n",
      "\tLOC: precision:  87.72%; recall:  87.10%; F1:  87.41 1824\n",
      "\n",
      "\tMISC: precision:  84.32%; recall:  78.74%; F1:  81.44 861\n",
      "\n",
      "\tORG: precision:  80.98%; recall:  70.17%; F1:  75.19 1162\n",
      "\n",
      "\tPER: precision:  87.12%; recall:  66.45%; F1:  75.39 1405\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 17:57:41.907 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5207 phrases; correct: 4526.\n",
      "\n",
      "precision:  86.92%; recall:  76.17%; FB1:  81.19\n",
      "\n",
      "\tLOC: precision:  92.55%; recall:  85.25%; F1:  88.75 1692\n",
      "\n",
      "\tMISC: precision:  85.35%; recall:  79.61%; F1:  82.38 860\n",
      "\n",
      "\tORG: precision:  80.36%; recall:  73.23%; F1:  76.63 1222\n",
      "\n",
      "\tPER: precision:  86.81%; recall:  67.54%; F1:  75.97 1433\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for x, y in data_iterator.gen_batches(batch_size, 'train'):\n",
    "        # Convert tokens to indices via Vocab\n",
    "        x_inds = token_vocab(x)# YOUR CODE \n",
    "        # Convert tags to indices via Vocab\n",
    "        y_inds = tag_vocab(y)# YOUR CODE \n",
    "        \n",
    "        # Pad every sample with zeros to the maximal length\n",
    "        x_batch = zero_pad(x_inds)\n",
    "        y_batch = zero_pad(y_inds)\n",
    "\n",
    "        mask = get_mask(x)\n",
    "        nernet.train_on_batch(x_batch, y_batch, mask, dropout_keep_prob, learning_rate)\n",
    "    print('Evaluating the model on valid part of the dataset')\n",
    "    eval_valid(nernet, data_iterator.gen_batches(batch_size, 'valid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval the model on test part now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_valid(nernet, data_iterator.gen_batches(batch_size, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to infer the model on our sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'Petr stole my vodka'\n",
    "x = [sentence.split()]\n",
    "\n",
    "x_inds = token_vocab(x)\n",
    "x_batch = zero_pad(x_inds)\n",
    "mask = get_mask(x)\n",
    "y_inds = nernet(x_batch, mask)\n",
    "print(x[0])\n",
    "print(tag_vocab(y_inds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
